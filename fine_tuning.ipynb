{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Schlager Bot\n",
    "\n",
    "Using the dataset of colelcted Schlager song lyrics, we will fine tune a German based LLM to generate song lyrics from a provided verse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Data into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "\n",
    "from random import randrange\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\"false,\"reason\"\"Unexpected error occurred (no quota cost) Please try again later\"}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics = pd.read_csv('schlager_songs.csv')\n",
    "\n",
    "df_lyrics['lyrics'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate column for 1st verse\n",
    "\n",
    "verse_list = []\n",
    "\n",
    "for ind in df_lyrics[\"lyrics\"].index:\n",
    "    lyrics = df_lyrics[\"lyrics\"].iloc[ind]\n",
    "    if lyrics == '{\"status\"false,\"reason\"\"Unexpected error occurred (no quota cost) Please try again later\"}' :\n",
    "        verse_list.append(None)\n",
    "    else:\n",
    "        try:\n",
    "            verse = lyrics.split(\"\\n\")[0]\n",
    "            n = 1\n",
    "            while len(' '.join(verse).split()) <= 15:\n",
    "                verse = lyrics.split(\"\\n\")[0:n]\n",
    "                n += 1\n",
    "            verse_list.append(''.join(verse))\n",
    "\n",
    "        except:\n",
    "            verse_list.append(None)\n",
    "\n",
    "\n",
    "df_lyrics[\"verse_1\"] = verse_list\n",
    "\n",
    "df_lyrics[\"verse_1\"][0]\n",
    "\n",
    "df_lyrics.to_csv(\"schlager_songs_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Du sagst nicht ein Wort\\n Und deine Hand wischt eine Träne fort\\n Und dein leerer Blick\\n Sinkt in dein Glas\\n Du sitzt hier vor mir\\n Und dein Gesicht lässt keinen Zweifel mehr\\n Heut sagst du mir\\n Dass ich dich verlier'\\n Nie war Zeit für dich\\n Ich lebte nur in meiner eignen Welt\\n Ich weiß, du wirst gehen\\n Ich muss dich versteh'n\\n Lieb mich ein letztes mal\\n Es bleibt mir keine andre Wahl\\n Ich weiß, dass ich die Nacht mit dir\\n An den Tag verlier'\\n...\\n Und bleib bei mir\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Example\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "\"Benuzte den unten gegebenen Vers um den Text für ein Lied zu schreiben.\"\n",
    "\n",
    "### Input:\n",
    "\n",
    "\"Du sagst nicht ein Wort\"\n",
    "\n",
    "### Response:\n",
    "\n",
    "f\"\"\" Du sagst nicht ein Wort\n",
    " Und deine Hand wischt eine Träne fort\n",
    " Und dein leerer Blick\n",
    " Sinkt in dein Glas\n",
    " Du sitzt hier vor mir\n",
    " Und dein Gesicht lässt keinen Zweifel mehr\n",
    " Heut sagst du mir\n",
    " Dass ich dich verlier'\n",
    " Nie war Zeit für dich\n",
    " Ich lebte nur in meiner eignen Welt\n",
    " Ich weiß, du wirst gehen\n",
    " Ich muss dich versteh'n\n",
    " Lieb mich ein letztes mal\n",
    " Es bleibt mir keine andre Wahl\n",
    " Ich weiß, dass ich die Nacht mit dir\n",
    " An den Tag verlier'\n",
    "...\n",
    " Und bleib bei mir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "Benuzte den unten gegebenen Vers um den Text für ein Lied zu schreiben.\n",
    "\n",
    "### Input:\n",
    "\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\n",
    "{sample['output']}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DatasetCard' from 'huggingface_hub' (c:\\Users\\nicla\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nicla\\coding_projects\\schlager-lyrics-bot\\fine_tuning.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicla/coding_projects/schlager-lyrics-bot/fine_tuning.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Create input and output dataset from dataframe\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nicla/coding_projects/schlager-lyrics-bot/fine_tuning.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicla/coding_projects/schlager-lyrics-bot/fine_tuning.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_dataset \u001b[39m=\u001b[39m df_lyrics[[\u001b[39m\"\u001b[39m\u001b[39mverse_1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlyrics\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicla/coding_projects/schlager-lyrics-bot/fine_tuning.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ind_nan \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\nicla\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\datasets\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2.14.5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrow_reader\u001b[39;00m \u001b[39mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\nicla\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\datasets\\arrow_dataset.py:61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpa\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompute\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpc\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetCard, DatasetCardData, HfApi, HfFolder\n\u001b[0;32m     62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m Pool\n\u001b[0;32m     63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPError\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DatasetCard' from 'huggingface_hub' (c:\\Users\\nicla\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#Create input and output dataset from dataframe\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "df_dataset = df_lyrics[[\"verse_1\", \"lyrics\"]]\n",
    "\n",
    "ind_nan = []\n",
    "\n",
    "for ind in df_dataset.index:\n",
    "    if df_dataset[\"verse_1\"].iloc[ind] == None:\n",
    "        ind_nan.append(ind)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "df_dataset.drop(index=ind_nan, inplace=True)\n",
    "\n",
    "df_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_dataset.rename(columns={\"lyrics\" : \"output\", \"verse_1\" : \"input\"}, inplace= True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_dataset)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Benuzte den unten gegebenen Vers um den Text für ein Lied zu schreiben.\n",
      "\n",
      "### Input:\n",
      "\n",
      " Du sagst nicht ein Wort\n",
      "\n",
      "### Response:\n",
      "\n",
      " Du sagst nicht ein Wort\n",
      " Und deine Hand wischt eine Träne fort\n",
      " Und dein leerer Blick\n",
      " Sinkt in dein Glas\n",
      " Du sitzt hier vor mir\n",
      " Und dein Gesicht lässt keinen Zweifel mehr\n",
      " Heut sagst du mir\n",
      " Dass ich dich verlier'\n",
      " Nie war Zeit für dich\n",
      " Ich lebte nur in meiner eignen Welt\n",
      " Ich weiß, du wirst gehen\n",
      " Ich muss dich versteh'n\n",
      " Lieb mich ein letztes mal\n",
      " Es bleibt mir keine andre Wahl\n",
      " Ich weiß, dass ich die Nacht mit dir\n",
      " An den Tag verlier'\n",
      " Schenk mir die Zeit\n",
      " Die uns noch bleibt\n",
      " Lieb mich ein letztes Mal\n",
      " Lass' mich dich noch einmal spür'n\n",
      " Bist du auch morgen nicht mehr hier\n",
      " Etwas bleibt von dir\n",
      " Wenn ich erwach'\n",
      " Alleine erwach'\n",
      " Ich hab' so viel Zeit\n",
      " Neben dir, an dir vorbei gelebt\n",
      " Ich nahm, doch was ich gab\n",
      " Zählte nicht viel\n",
      " Du hast oft geweint\n",
      " Doch ich war blind\n",
      " Sah deine Tränen nicht\n",
      " Heut ist mir klar\n",
      " Wie einsam du warst\n",
      " Lieb mich ein letztes mal\n",
      " Es bleibt mir keine andre Wahl\n",
      " Ich weiß, dass ich die Nacht mit dir\n",
      " An den Tag verlier'\n",
      " Schenk mir die Zeit\n",
      " Die uns noch bleibt\n",
      " Lieb mich ein letztes Mal\n",
      " Lass mich dich noch einmal spür'n\n",
      " Wenn du mich noch immer liebst\n",
      " Und du mir vergibst\n",
      " Dann schließ die Tür\n",
      " Und bleib bei mir\n",
      " Lieb mich ein letztes Mal\n",
      " Lass mich dich noch einmal spür'n\n",
      " Wenn du mich noch immer liebst\n",
      " Und du mir vergibst\n",
      " Dann schließ die Tür\n",
      " Und bleib bei mir\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fromat_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction-tune model using trl and the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"philschmid/instruct-igel-001\" # non-gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SFTTrainer supports a native integration with peft, which makes it easy to instruction tune LLMs\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to define the hyperparameters before we can start training\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"igel_schlager_001\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size= 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now configure the trainer\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model and run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following training we want to run and test our model\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "args.output_dir = \"igel_schlager_001\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with a random sample and try to generate the lyrics\n",
    "\n",
    "#TODO relaod dataset and sample\n",
    "\n",
    "sample = randrange(len(dataset))\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch._C._cuda_getCompiledVersion())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
