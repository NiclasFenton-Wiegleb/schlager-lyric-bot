{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNld1FG2gLrK8/AZVaWWpQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiclasFenton-Wiegleb/schlager-lyrics-bot/blob/main/Schlager_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Schlager Bot Language Model\n",
        "\n",
        "To train the model that will ultimately generate Schlager lyrics, we first train a BERT model from scratch on our text dataset composed of Schlager lyrics and some generic German text from an open source NLP training dataset. The latter serves to provide grammatical structure and vocabulary that the lyrics alone lack.\n",
        "\n",
        "Pre-training on transformers can be done with self-supervised tasks. In this case we will use Masked Language Modeling (MLM), where a certain percentage of the tokens in a sentence is masked and the model is trained to predict those masked words. One of the advantages of this method is that it can see the position information of the whole sentence - both for the masked and visible part.\n",
        "\n",
        "First we need to import the relevant dependencies."
      ],
      "metadata": {
        "id": "JjyznsSwdyfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import json\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "import datasets\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizerFast\n",
        "from itertools import chain"
      ],
      "metadata": {
        "id": "LhdTnkt_d0KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start putting our model together, we need to preprocess the training data and split it into training and test sub-sets. Separating these out first avoids the model seeing the test data during training and risking overfitting."
      ],
      "metadata": {
        "id": "2j4UY_80d24o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the lyrics data as the dataset\n",
        "\n",
        "#Iterating through the files in the directory and adding the names to the files variable\n",
        "\n",
        "files = []\n",
        "\n",
        "directory = \"./lyrics\"\n",
        "\n",
        "for filename in os.scandir(directory):\n",
        "    if filename.is_file():\n",
        "        files.append(filename.path)\n",
        "\n",
        "dataset = datasets.load_dataset(\"text\", data_files= files, split= \"train\")"
      ],
      "metadata": {
        "id": "U9hIFSvtd81q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we split the dataset into training (90%) and testing (10%)\n",
        "\n",
        "d = dataset.train_test_split(test_size= 0.1)\n",
        "d[\"train\"], d[\"test\"]"
      ],
      "metadata": {
        "id": "SCGs-YEFd_zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the dataset is loaded and split into training and test data, it is time to train the tokenizer. To achieve this, we need to write our dataset into text files - keeping training and test data separate."
      ],
      "metadata": {
        "id": "_WZQsy21eBv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
        "  \"\"\"Utility function to save dataset text to disk,\n",
        "  useful for using the texts to train the tokenizer\n",
        "  (as the tokenizer accepts files)\"\"\"\n",
        "  with open(output_filename, \"w\") as f:\n",
        "    for t in dataset[\"text\"]:\n",
        "      print(t, file=f)\n",
        "\n",
        "# save the training set to train.txt\n",
        "dataset_to_text(d[\"train\"], \"./model/model_data/train.txt\")\n",
        "# save the testing set to test.txt\n",
        "dataset_to_text(d[\"test\"], \"./model/model_data/test.txt\")"
      ],
      "metadata": {
        "id": "SWdYcn7WeE_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define some parameters of the tokenizer. The training file indicates the data we're passing to the tokenizer for training. This could be a list of files too. vocab_size is the vocabulary size of tokens, while max_length is the maximum sequence length.\n",
        "\n",
        "truncate_longer_samples is a boolean indicating whether we truncate sentences longer than the length of max_length, if it's set to False, we won't truncate the sentences, we group them together and split them by max_length, so all the resulting sentences will have the length of max_length."
      ],
      "metadata": {
        "id": "HCND6DqGeJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\n",
        "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
        "]\n",
        "\n",
        "training_file = [\"./model/model_data/train.txt\"]\n",
        "# 30,522 vocab is BERT's default vocab size\n",
        "vocab_size = 30_522\n",
        "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
        "max_length = 512\n",
        "# whether to truncate\n",
        "truncate_longer_samples = False"
      ],
      "metadata": {
        "id": "1uqic0QVeKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train the tokenizer. BERT's default tokenizer is WordPiece and, therefore, we initialize the BertWordPieceTokenizer() tokenizer class from the tokenizers library and use the train() method to train it. It will take several minutes to finish. We then need to save the tokenizer to a directory."
      ],
      "metadata": {
        "id": "wzoV0_eieMQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the WordPiece tokenizer\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "#Train the tokenizer\n",
        "tokenizer.train(files=training_file, vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "\n",
        "#Enable truncation up to the maximum 512 tokens\n",
        "# tokenizer.enable_truncation(max_length=max_length)\n",
        "\n",
        "model_path = \"./model/tokenizer/\"\n",
        "\n",
        "#Save the tokenizer to directory\n",
        "tokenizer.save_model(model_path)\n",
        "\n",
        "#Dumping some of the tokenizer config to config file,\n",
        "#including special tokens, whether to lower case and the maximum sequence length\n",
        "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
        "  tokenizer_cfg = {\n",
        "      \"do_lower_case\": True,\n",
        "      \"unk_token\": \"[UNK]\",\n",
        "      \"sep_token\": \"[SEP]\",\n",
        "      \"pad_token\": \"[PAD]\",\n",
        "      \"cls_token\": \"[CLS]\",\n",
        "      \"mask_token\": \"[MASK]\",\n",
        "      \"model_max_length\": max_length,\n",
        "      \"max_len\": max_length,\n",
        "  }\n",
        "  json.dump(tokenizer_cfg, f)"
      ],
      "metadata": {
        "id": "6QDv9NTxePc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer.save_model() method saves the vocabulary file into that path, we also manually save some tokenizer configurations, such as special tokens:\n",
        "\n",
        "- unk_token: A special token that represents an out-of-vocabulary token, even though the tokenizer is a WordPiece tokenizer, the unk tokens are not\n",
        "- impossible, but rare.\n",
        "- sep_token: A special token that separates two different sentences in the same input.\n",
        "- pad_token: A special token that is used to fill sentences that do not reach the maximum sequence length (since the arrays of tokens must be the same size).\n",
        "- cls_token: A special token representing the class of the input.\n",
        "- mask_token: This is the mask token we use for the Masked Language Modeling (MLM) pretraining task."
      ],
      "metadata": {
        "id": "og9OwJETeR_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's load the tokenizer\n",
        "model_path = \"./model/tokenizer/\"\n",
        "tokenizer = transformers.BertTokenizerFast.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "pibboiGMeVbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the tokenizer ready to be taken into operation, we can now tokenize our dataset."
      ],
      "metadata": {
        "id": "TtAhozdveX1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load train and test data\n",
        "train_data = datasets.load_dataset(\"text\", data_files= \"./model/model_data/train.txt\", split= \"train\")\n",
        "test_data = datasets.load_dataset(\"text\", data_files= \"./model/model_data/test.txt\", split= \"train\")\n",
        "#Tokenizing the training dataset\n",
        "train_dataset = train_data.map((lambda x: tokenizer(x[\"text\"], return_special_tokens_mask=True)), batched= True)\n",
        "\n",
        "#Tokenizing the test dataset\n",
        "test_dataset = test_data.map((lambda x: tokenizer(x[\"text\"], return_special_tokens_mask=True)), batched= True)"
      ],
      "metadata": {
        "id": "jQrKel8PebZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove other columns, and remain them as Python lists\n",
        "\n",
        "test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
        "train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
      ],
      "metadata": {
        "id": "qvLXEX4Gedc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
        "# max_seq_length.\n",
        "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
        "\n",
        "max_length = 256\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= max_length:\n",
        "        total_length = (total_length // max_length) * max_length\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "train_dataset = train_dataset.map(group_texts, batched=True,\n",
        "                                desc=f\"Grouping texts in chunks of {max_length}\")\n",
        "test_dataset = test_dataset.map(group_texts, batched=True,\n",
        "                                desc=f\"Grouping texts in chunks of {max_length}\")"
      ],
      "metadata": {
        "id": "O4W8ksKXef6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert them from lists to torch tensors\n",
        "train_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "1sxmgRUmeiiq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}